


# Adversarial Learning in Natural Language Generation

## Overview
This project focuses on advancing Natural Language Generation (NLG) techniques through the integration of cutting-edge technologies, specifically Transformers and Generative Adversarial Networks (GANs). The goal is to develop a state-of-the-art NLG system capable of generating highly fluent and contextually relevant text.

## Achievements

### Transformer-GAN Integration

- **Transformer-GAN Fusion:** We have successfully integrated the Transformer architecture with Generative Adversarial Networks (GANs). This hybrid approach combines the strengths of both models to enhance text generation capabilities.

### Benchmark Outperformance

- **WMT 2016 English-German Data:** Our NLG system has demonstrated remarkable performance on the WMT 2016 English-German dataset, surpassing existing benchmarks.

  Tranformer based on https://github.com/IpsumDominum/Pytorch-Simple-Transformer

  Data for WMT2014 https://www.statmt.org/europarl/v7/fr-en.tgz


- **BLEU Score:** We achieved an impressive BLEU score of 29.74, indicating the high quality and fluency of the generated text.

- **METEOR Score:** Our system also excelled in terms of METEOR, achieving a score of 62.62, demonstrating the system's ability to generate text that is highly relevant to the context.

### Token-wise LSTM Discriminator

- **Discriminator Enhancement:** To further boost the quality of our generated text, we introduced a token-wise Long Short-Term Memory (LSTM) discriminator. This component evaluates and refines the text generated by the GAN, resulting in text that is even more contextually accurate.

- **Improved BLEU:** With the inclusion of the token-wise LSTM discriminator, we achieved an additional gain of 1.58 points in the BLEU score. This enhancement underscores the effectiveness of our approach.


## Contact
For any queries or suggestions, please reach out to me at:vishalpanda10@gmail.com.

